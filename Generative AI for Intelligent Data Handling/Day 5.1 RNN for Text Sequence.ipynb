{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9beb830",
   "metadata": {},
   "source": [
    "## RNN is used to generate a sequence of text, such as generating text character by character. \n",
    "\n",
    "- A simple Recurrent Neural Network (RNN) can be used for text data sequence generation in various ways. Here are a few examples:\n",
    "\n",
    "## Character-level Text Generation:\n",
    "\n",
    "### Generate text character by character.\n",
    "Example: Predict the next character in a sequence based on previous characters.\n",
    "- Application: Generating new text in the style of a given text corpus (e.g., generating new Shakespearean text).\n",
    "Word-level Text Generation:\n",
    "\n",
    "#### Generate text word by word.\n",
    "Example: Predict the next word in a sequence based on previous words.\n",
    "- Application: Creating coherent sentences or paragraphs based on a training corpus (e.g., generating news headlines).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6020ad08",
   "metadata": {},
   "source": [
    "### Example 1: Predicting the Next Character in a String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d2c7b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Generate a sequence of characters\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "sequence_length = len(alphabet)\n",
    "\n",
    "# Prepare data\n",
    "X = []\n",
    "y = []\n",
    "for i in range(sequence_length - 4):\n",
    "    X.append([ord(char) for char in alphabet[i:i+4]])\n",
    "    y.append(ord(alphabet[i+4]))\n",
    "\n",
    "X = np.array(X).reshape((-1, 4, 1)) / 255.0  # Normalize\n",
    "y = to_categorical(y, num_classes=256)  # One-hot encoding\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae680bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bde3d4b940>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define RNN model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(50, activation='relu', input_shape=(4, 1)))\n",
    "model.add(Dense(256, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=1000, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b087787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new sequence\n",
    "input_sequence = np.array([ord(char) for char in \"abcd\"]).reshape((1, 4, 1)) / 255.0\n",
    "prediction = model.predict(input_sequence, verbose=0)\n",
    "predicted_char = chr(np.argmax(prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d502e338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.38039216],\n",
       "        [0.38431373],\n",
       "        [0.38823529],\n",
       "        [0.39215686]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98679228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence: abcd\n",
      "Next Character Prediction: e\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(\"Input Sequence:\", \"abcd\")\n",
    "print(\"Next Character Prediction:\", predicted_char)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96512e45",
   "metadata": {},
   "source": [
    "### Example 2: Generate text word by word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ddd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "\n",
    "# Example input text\n",
    "text = \"how are you feeling today?\"\n",
    "\n",
    "# Create character mapping\n",
    "chars = sorted(set(text))\n",
    "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "idx_to_char = {idx: ch for idx, ch in enumerate(chars)}\n",
    "num_chars = len(chars)\n",
    "\n",
    "# Prepare input-output pairs for training\n",
    "max_len = 10  # Adjust max_len based on the sequences used for training\n",
    "step = 1\n",
    "sequences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - max_len, step):\n",
    "    sequences.append(text[i:i + max_len])\n",
    "    next_chars.append(text[i + max_len])\n",
    "    \n",
    "# Vectorization\n",
    "X = np.zeros((len(sequences), max_len, num_chars), dtype=np.float32)\n",
    "y = np.zeros((len(sequences), num_chars), dtype=np.float32)\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        X[i, t, char_to_idx[char]] = 1.0\n",
    "    y[i, char_to_idx[next_chars[i]]] = 1.0\n",
    "\n",
    "# Build the RNN model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(max_len, num_chars)))\n",
    "model.add(Dense(num_chars, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Training the model\n",
    "model.fit(X, y, batch_size=1, epochs=100, verbose=2)\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(model, seed_text, max_len, num_chars):\n",
    "    generated_text = seed_text\n",
    "    for _ in range(max_len):\n",
    "        x_pred = np.zeros((1, max_len, num_chars), dtype=np.float32)\n",
    "        for t, char in enumerate(seed_text):\n",
    "            x_pred[0, t, char_to_idx[char]] = 1.0\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = np.random.choice(num_chars, p=preds)\n",
    "        next_char = idx_to_char[next_index]\n",
    "        generated_text += next_char\n",
    "        seed_text = seed_text[1:] + next_char\n",
    "    return generated_text\n",
    "\n",
    "# Generate a sequence\n",
    "generated_sequence = generate_text(model, seed_text=\"hello \", max_len=10, num_chars=num_chars)  # Use max_len consistent with training\n",
    "print(\"Generated Sequence:\")\n",
    "print(generated_sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c192aa6",
   "metadata": {},
   "source": [
    "## 3.Example Word sequence Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "695828e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 2.1941 - accuracy: 0.2857\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.1915 - accuracy: 0.2857\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.1889 - accuracy: 0.2857\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.1862 - accuracy: 0.4286\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.1835 - accuracy: 0.4286\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.1806 - accuracy: 0.2857\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.1775 - accuracy: 0.2857\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1744 - accuracy: 0.1429\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.1710 - accuracy: 0.1429\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.1675 - accuracy: 0.1429\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1638 - accuracy: 0.1429\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.1598 - accuracy: 0.1429\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.1556 - accuracy: 0.2857\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.1511 - accuracy: 0.2857\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1464 - accuracy: 0.2857\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.1413 - accuracy: 0.2857\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.1358 - accuracy: 0.2857\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.1300 - accuracy: 0.2857\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.1238 - accuracy: 0.2857\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.1171 - accuracy: 0.2857\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.1099 - accuracy: 0.2857\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.1021 - accuracy: 0.2857\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.0938 - accuracy: 0.1429\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0848 - accuracy: 0.1429\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.0751 - accuracy: 0.2857\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.0646 - accuracy: 0.2857\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.0533 - accuracy: 0.2857\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.0411 - accuracy: 0.2857\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.0279 - accuracy: 0.2857\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0137 - accuracy: 0.4286\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9983 - accuracy: 0.4286\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9818 - accuracy: 0.4286\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9640 - accuracy: 0.4286\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.9450 - accuracy: 0.4286\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.9246 - accuracy: 0.4286\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9028 - accuracy: 0.4286\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8797 - accuracy: 0.4286\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.8553 - accuracy: 0.4286\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8296 - accuracy: 0.4286\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8027 - accuracy: 0.4286\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.7746 - accuracy: 0.2857\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.7454 - accuracy: 0.2857\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.7150 - accuracy: 0.4286\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.6833 - accuracy: 0.4286\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.6504 - accuracy: 0.4286\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.6161 - accuracy: 0.4286\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.5806 - accuracy: 0.4286\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.5441 - accuracy: 0.4286\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.5071 - accuracy: 0.5714\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.4698 - accuracy: 0.5714\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.4328 - accuracy: 0.7143\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.3961 - accuracy: 0.5714\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3600 - accuracy: 0.5714\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.3244 - accuracy: 0.5714\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2894 - accuracy: 0.5714\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.2551 - accuracy: 0.5714\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2217 - accuracy: 0.5714\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1891 - accuracy: 0.5714\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1572 - accuracy: 0.5714\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.1258 - accuracy: 0.7143\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0948 - accuracy: 0.8571\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0643 - accuracy: 0.8571\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0344 - accuracy: 0.8571\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0050 - accuracy: 0.8571\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9761 - accuracy: 0.8571\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.9476 - accuracy: 0.8571\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9199 - accuracy: 0.8571\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8928 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8664 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8404 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8149 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7898 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7650 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7407 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7171 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6943 - accuracy: 0.8571\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6728 - accuracy: 0.8571\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6529 - accuracy: 0.8571\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6348 - accuracy: 0.8571\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6186 - accuracy: 0.8571\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6036 - accuracy: 0.8571\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5894 - accuracy: 0.8571\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5756 - accuracy: 0.8571\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5619 - accuracy: 0.8571\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5482 - accuracy: 0.8571\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5346 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5211 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5079 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4953 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4832 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4718 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4610 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4507 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4407 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4306 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4204 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4098 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.3992 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3887 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.3788 - accuracy: 1.0000\n",
      "Hi Rubi what are you doing today\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Example text data (replace with your dataset)\n",
    "text_data = [\n",
    "    \"Hi what are you doing today? Any plans\"\n",
    "]\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Create input sequences using tokenizer\n",
    "input_sequences = []\n",
    "for line in text_data:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Pad sequences for equal length input\n",
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Create predictors and labels\n",
    "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "label = tf.keras.utils.to_categorical(label, num_classes=total_words)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 10, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(predictors, label, epochs=100, verbose=1)\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted_probs = model.predict(token_list, verbose=0)[0]\n",
    "        predicted_index = np.argmax(predicted_probs)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_index:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n",
    "\n",
    "# Generate text\n",
    "generated_text = generate_text(\"Hi Rubi\", 5, model, max_sequence_len)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17d0be07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 3.1357 - accuracy: 0.0800\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.1338 - accuracy: 0.1200\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.1318 - accuracy: 0.1600\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.1299 - accuracy: 0.1200\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.1279 - accuracy: 0.1200\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.1258 - accuracy: 0.1200\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.1237 - accuracy: 0.1200\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.1214 - accuracy: 0.0800\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.1190 - accuracy: 0.0800\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.1165 - accuracy: 0.0800\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 3.1137 - accuracy: 0.0800\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.1108 - accuracy: 0.0800\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.1076 - accuracy: 0.0800\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.1041 - accuracy: 0.0800\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.1002 - accuracy: 0.0800\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.0960 - accuracy: 0.0800\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.0912 - accuracy: 0.0800\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.0860 - accuracy: 0.0800\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.0802 - accuracy: 0.0800\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.0737 - accuracy: 0.0800\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.0664 - accuracy: 0.0800\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.0584 - accuracy: 0.0800\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.0494 - accuracy: 0.0800\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.0397 - accuracy: 0.0800\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.0293 - accuracy: 0.0800\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.0186 - accuracy: 0.0800\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.0084 - accuracy: 0.0800\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.9995 - accuracy: 0.0800\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.9929 - accuracy: 0.0800\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.9883 - accuracy: 0.0800\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.9841 - accuracy: 0.0800\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.9784 - accuracy: 0.0800\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.9708 - accuracy: 0.0800\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.9619 - accuracy: 0.0800\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.9527 - accuracy: 0.0800\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.9442 - accuracy: 0.1200\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.9367 - accuracy: 0.1200\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.9299 - accuracy: 0.1600\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.9232 - accuracy: 0.1200\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.9162 - accuracy: 0.1200\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.9085 - accuracy: 0.1600\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.8999 - accuracy: 0.1200\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.8903 - accuracy: 0.1600\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.8799 - accuracy: 0.2000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.8689 - accuracy: 0.2000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.8575 - accuracy: 0.2000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.8454 - accuracy: 0.2000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.8323 - accuracy: 0.2000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.8175 - accuracy: 0.1200\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.8009 - accuracy: 0.1200\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.7826 - accuracy: 0.1600\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.7633 - accuracy: 0.1600\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.7431 - accuracy: 0.1600\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.7220 - accuracy: 0.1600\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.6993 - accuracy: 0.1600\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.6750 - accuracy: 0.1600\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.6495 - accuracy: 0.1600\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.6239 - accuracy: 0.1600\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.5985 - accuracy: 0.1600\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.5723 - accuracy: 0.2000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.5456 - accuracy: 0.1600\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.5193 - accuracy: 0.1600\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.4925 - accuracy: 0.1600\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.4650 - accuracy: 0.2000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.4387 - accuracy: 0.2400\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.4116 - accuracy: 0.2000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.3857 - accuracy: 0.2000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3587 - accuracy: 0.2400\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.3329 - accuracy: 0.2000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.3063 - accuracy: 0.2000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.2784 - accuracy: 0.2000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.2507 - accuracy: 0.2000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.2249 - accuracy: 0.2000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.2061 - accuracy: 0.2400\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.1887 - accuracy: 0.2800\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.1485 - accuracy: 0.2800\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.1572 - accuracy: 0.2400\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1033 - accuracy: 0.3200\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.1072 - accuracy: 0.3600\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.0536 - accuracy: 0.3200\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0820 - accuracy: 0.2800\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.0184 - accuracy: 0.2800\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.0337 - accuracy: 0.3600\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 9ms/step - loss: 1.9902 - accuracy: 0.3200\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9831 - accuracy: 0.2800\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.9513 - accuracy: 0.3200\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.9403 - accuracy: 0.4000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.9273 - accuracy: 0.4000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.8945 - accuracy: 0.3600\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.8983 - accuracy: 0.2400\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8664 - accuracy: 0.4000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.8640 - accuracy: 0.4000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.8287 - accuracy: 0.4400\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.8356 - accuracy: 0.3600\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.8069 - accuracy: 0.4000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.8005 - accuracy: 0.4000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.7725 - accuracy: 0.4000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.7639 - accuracy: 0.3600\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.7551 - accuracy: 0.4000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.7319 - accuracy: 0.3600\n",
      "Enter a starting phrase: Delhi\n",
      "Enter number of words to generate: 30\n",
      "Generated Text: delhi sells sells sells much the the the the the the dog dog dog dog could could could chuck could chuck could could could could could could could could could could\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Example text data (replace with your dataset)\n",
    "text_data = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"She sells seashells by the seashore.\",\n",
    "    \"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\"\n",
    "]\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Create input sequences using tokenizer\n",
    "input_sequences = []\n",
    "for line in text_data:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Pad sequences for equal length input\n",
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Create predictors and labels\n",
    "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "label = tf.keras.utils.to_categorical(label, num_classes=total_words)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 10, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(predictors, label, epochs=100, verbose=1)\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted_probs = model.predict(token_list, verbose=0)[0]\n",
    "        predicted_index = np.argmax(predicted_probs)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_index:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n",
    "\n",
    "# User input to generate text\n",
    "user_input = input(\"Enter a starting phrase: \")\n",
    "num_words = int(input(\"Enter number of words to generate: \"))\n",
    "\n",
    "generated_text = generate_text(user_input.lower(), num_words, model, max_sequence_len)\n",
    "print(\"Generated Text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195fc8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
