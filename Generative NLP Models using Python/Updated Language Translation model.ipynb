{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3a30267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select the language to translate English into:\n",
      "1. Hindi\n",
      "2. German\n",
      "3. French\n",
      "4. Spanish\n",
      "Enter choice (1-4): 2\n",
      "Enter English text to translate into German: living is good\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-base and revision 686f1db (https://huggingface.co/google-t5/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\Suyashi144893\\AppData\\Local\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c96c1acb1f142bfa34461e2cc1618cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Suyashi144893\\AppData\\Local\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Suyashi144893\\.cache\\huggingface\\hub\\models--google-t5--t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Suyashi144893\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37fe8f9ee117459b980c8aa77b081f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4373ab9e303d47799db2d9d9b3f327da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce7e9bed77344098ad3fdc1751efef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a62612de2b843f1995ff41077776241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translated text in German: Das Leben ist gut\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Step 1: Define supported target languages and model\n",
    "supported_languages = {\n",
    "    \"2\": (\"German\", \"translation_en_to_de\"),\n",
    "    \"3\": (\"French\", \"translation_en_to_fr\"),\n",
    "    \"4\": (\"Spanish\", \"translation_en_to_es\"),\n",
    "}\n",
    "\n",
    "# Step 2: Display menu to user\n",
    "print(\"Select the language to translate English into:\")\n",
    "for key, (language, _) in supported_languages.items():\n",
    "    print(f\"{key}. {language}\")\n",
    "\n",
    "# Step 3: Take user input for language and text\n",
    "lang_choice = input(\"Enter choice (1-4): \").strip()\n",
    "if lang_choice not in supported_languages:\n",
    "    print(\"Invalid choice. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "target_lang, pipeline_task = supported_languages[lang_choice]\n",
    "\n",
    "text_to_translate = input(f\"Enter English text to translate into {target_lang}: \")\n",
    "\n",
    "# Step 4: Load appropriate model and tokenizer\n",
    "translator = pipeline(pipeline_task)\n",
    "\n",
    "# Step 5: Translate and display result\n",
    "result = translator(text_to_translate)\n",
    "print(f\"\\nTranslated text in {target_lang}: {result[0]['translation_text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fb3100",
   "metadata": {},
   "source": [
    "## Second Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f8f87f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select target language:\n",
      "1. Hindi\n",
      "2. German\n",
      "3. French\n",
      "4. Spanish\n",
      "Enter your choice (1-4): 2\n",
      "Enter English text to translate to German: hello hey\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ac10717ee94c5c885deaf46a712950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Suyashi144893\\AppData\\Local\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Suyashi144893\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-de. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebed631ba9dd40fe9d158cddf5100cb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/768k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b305c7388cb4c1ea87b58f4434707d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/797k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2d094f23bb4f6a9af94b86ec2f52ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190e07b2114c41ffa5651913ca5f6a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f250b863aa42a382346b04d9075547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/298M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f717e27ef84edd964cb021cab7318d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Suyashi144893\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4072: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translated to German: Guten Tag.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianTokenizer, MarianMTModel\n",
    "\n",
    "# Language map\n",
    "language_models = {\n",
    "    \"1\": (\"Hindi\", \"Helsinki-NLP/opus-mt-en-hi\"),\n",
    "    \"2\": (\"German\", \"Helsinki-NLP/opus-mt-en-de\"),\n",
    "    \"3\": (\"French\", \"Helsinki-NLP/opus-mt-en-fr\"),\n",
    "    \"4\": (\"Spanish\", \"Helsinki-NLP/opus-mt-en-es\"),\n",
    "}\n",
    "\n",
    "# Show menu\n",
    "print(\"Select target language:\")\n",
    "for key, (lang, _) in language_models.items():\n",
    "    print(f\"{key}. {lang}\")\n",
    "\n",
    "# Get input\n",
    "choice = input(\"Enter your choice (1-4): \").strip()\n",
    "if choice not in language_models:\n",
    "    print(\"Invalid selection.\")\n",
    "    exit()\n",
    "\n",
    "lang_name, model_name = language_models[choice]\n",
    "text = input(f\"Enter English text to translate to {lang_name}: \")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Translate\n",
    "tokens = tokenizer.prepare_seq2seq_batch([text], return_tensors=\"pt\")\n",
    "translated = model.generate(**tokens)\n",
    "output = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "# Result\n",
    "print(f\"\\nTranslated to {lang_name}: {output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f96be7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
