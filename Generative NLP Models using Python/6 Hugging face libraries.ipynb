{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e6b689a8",
   "metadata": {},
   "source": [
    "pip install --upgrade numpy\n",
    "pip install transformer\n",
    "conda create -n huggingface_env python=3.10 -y\n",
    "conda activate huggingface_env\n",
    "pip install huggingface_hub[hf_xet]\n",
    "pip install numpy==1.26.2 tensorflow==2.18.0 transformers\n",
    "pip install tensorflow-intel==2.18.0 numpy==1.26.2\n",
    "pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a30945f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13923ea2",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers for NLP Tasks\n",
    "\n",
    "---\n",
    "\n",
    "## What is Hugging Face?\n",
    "\n",
    "[Hugging Face](https://huggingface.co/) provides:\n",
    "\n",
    "- `transformers`: State-of-the-art pre-trained models for NLP (and beyond).\n",
    "- `datasets`: Ready-to-use NLP datasets.\n",
    "- `tokenizers`: Fast and customizable tokenization.\n",
    "\n",
    "---\n",
    "\n",
    "## Installation\n",
    "\n",
    "pip install transformers\n",
    "pip install torch  # or tensorflow, depending on backend\n",
    "\n",
    "\n",
    "\n",
    "## Common NLP Tasks with Hugging Face\n",
    "\n",
    "| Task                           | Description                               | Model Example       |\n",
    "| ------------------------------ | ----------------------------------------- | ------------------- |\n",
    "| Text Classification            | Classify text into categories             | BERT, DistilBERT    |\n",
    "| Named Entity Recognition (NER) | Identify entities in text                 | BERT, RoBERTa       |\n",
    "| Question Answering             | Extract answer from a given context       | BERT, ALBERT        |\n",
    "| Summarization                  | Summarize long text into key points       | BART, T5            |\n",
    "| Translation                    | Translate text between languages          | MarianMT, mBART     |\n",
    "| Text Generation                | Autocomplete or continue text generation  | GPT-2, GPT-3, LLaMA |\n",
    "| Sentiment Analysis             | Detect sentiment (positive/negative/etc.) | DistilBERT, BERT    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31019565",
   "metadata": {},
   "source": [
    "**The pipeline() function is the simplest way to use Hugging Face models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd69f2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Suyashi144893\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\Suyashi144893\\AppData\\Local\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9959076642990112}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Explicitly specify the model name\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# Load the sentiment analysis pipeline with the specified model\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model_name, tokenizer=model_name)\n",
    "\n",
    "# Sentiment analysis\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "print(classifier(\"Suyashi hates coding\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6626f1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\Suyashi144893\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:168: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9978213, 'word': 'Ashi', 'start': 11, 'end': 15}, {'entity_group': 'LOC', 'score': 0.9997433, 'word': 'India', 'start': 30, 'end': 35}]\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition\n",
    "ner = pipeline(\"ner\", model=\"dslim/bert-base-NER\")\n",
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "print(ner(\"My name is Ashi and I live in India.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7db22e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03b64c797034dc3b4d39523856de124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Suyashi144893\\AppData\\Local\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Suyashi144893\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-cased-distilled-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5984ac2103ce48868c4053ed7122b9c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0206df98aef54e15bc761f8e6637b652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0caeee06a8f243b7aa8c7588570e48b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4468b53928c4dd687ae1b9c45ca9bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9746261239051819, 'start': 30, 'end': 35, 'answer': 'India'}\n"
     ]
    }
   ],
   "source": [
    "# Question Answering\n",
    "qa = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"distilbert/distilbert-base-cased-distilled-squad\",\n",
    "    revision=\"564e9b5\"  # Optional: use this to lock the exact version\n",
    ")\n",
    "qa = pipeline(\"question-answering\")\n",
    "result = qa({\n",
    "    'question': 'Where do I live?',\n",
    "    'context': 'My name is Ashi and I live in India.'\n",
    "})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ebf7ede",
   "metadata": {},
   "source": [
    "# Summarization\n",
    "summarizer = pipeline(\"summarization\")\n",
    "text = \"\"\"Life is a journey, a path unknown,\n",
    "A road we walk, yet not alone.\n",
    "With dreams as maps and hope as light,\n",
    "We trek through day and rest by night.\n",
    "\n",
    "Some steps are smooth, some steep and rough,\n",
    "At times the way feels long and tough.\n",
    "But every turn and twist we face,\n",
    "Reveals new strength, unveils new grace.\n",
    "\n",
    "We meet some souls who walk awhile,\n",
    "They teach us love, they make us smile.\n",
    "And others pass like fleeting air,\n",
    "Yet leave a mark that lingers there.\n",
    "\n",
    "So walk with courage, heart held high,\n",
    "Beneath the storm or open sky.\n",
    "For life’s a journey — not the end, but why..\"\"\"\n",
    "print(summarizer(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12544443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le travail est l'emploi\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"t5-base\"\n",
    "\n",
    "translator = pipeline(\n",
    "    \"translation_en_to_fr\",\n",
    "    model=AutoModelForSeq2SeqLM.from_pretrained(model_name),\n",
    "    tokenizer=AutoTokenizer.from_pretrained(model_name),\n",
    "    revision=\"a9723ea\",  # Optional, to pin model version\n",
    "    device=-1  # Force CPU; use device=0 for GPU\n",
    ")\n",
    "\n",
    "# Test\n",
    "text = \"Work is workship\"\n",
    "result = translator(text)\n",
    "print(result[0]['translation_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e3de60",
   "metadata": {},
   "source": [
    "### Under the Hood: Model & Tokenizer Loading\n",
    "You can manually load the model/tokenizer if you want finer control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5a271ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "inputs = tokenizer(\"I love this product!\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "prediction = torch.argmax(logits)\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bab40dd",
   "metadata": {},
   "source": [
    "## Advantages of Hugging Face Pipelines\n",
    "Quick setup\n",
    "\n",
    "Pre-trained models ready for use\n",
    "\n",
    "Handles tokenization, model inference, and decoding internally\n",
    "\n",
    "Easy to switch models by changing model name\n",
    "\n",
    "### Where to Find Models\n",
    "- Browse thousands of models at https://huggingface.co/models\n",
    "\n",
    "## Summary\n",
    "- Hugging Face makes using SOTA NLP models easy with pipeline().\n",
    "\n",
    "- Supports a wide variety of tasks: classification, NER, QA, summarization, etc.\n",
    "\n",
    "- You can dig deeper by using tokenizers and model classes directly.\n",
    "\n",
    "- Easily switch between models with AutoModel and AutoTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60facd15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
