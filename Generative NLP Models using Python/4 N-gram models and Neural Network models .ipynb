{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b14c62b",
   "metadata": {},
   "source": [
    "The **N-Gram model** is a probabilistic language model used to predict the next item in a sequence, usually a word or character, based on the previous **N-1** items.\n",
    "\n",
    "## Types\n",
    "\n",
    "### ðŸ“˜ 1. **Unigram Model (N = 1)**\n",
    "\n",
    "Only considers the probability of each word independently.\n",
    "\n",
    "**Corpus:**\n",
    "`\"I love ice cream\"`\n",
    "\n",
    "**Unigram probabilities (based on frequency):**\n",
    "\n",
    "* P(I) = 1/4\n",
    "* P(love) = 1/4\n",
    "* P(ice) = 1/4\n",
    "* P(cream) = 1/4\n",
    "\n",
    "**Usage:**\n",
    "To generate text, choose each word based on its unigram probability, ignoring previous words.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“˜ 2. **Bigram Model (N = 2)**\n",
    "\n",
    "Considers the probability of a word based on the previous word.\n",
    "\n",
    "**Corpus:**\n",
    "`\"I love ice cream\"`\n",
    "\n",
    "**Bigrams and probabilities:**\n",
    "\n",
    "* P(love | I) = 1\n",
    "* P(ice | love) = 1\n",
    "* P(cream | ice) = 1\n",
    "\n",
    "**To calculate a sentence probability:**\n",
    "- P(I love ice cream) = P(I) Ã— P(love | I) Ã— P(ice | love) Ã— P(cream | ice)\n",
    "\n",
    "- Letâ€™s assume P(I) = 0.25 (from unigram), then:\n",
    "P(sentence) = 0.25 Ã— 1 Ã— 1 Ã— 1 = **0.25**\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“˜ 3. **Trigram Model (N = 3)**\n",
    "\n",
    "Considers two previous words to predict the next.\n",
    "\n",
    "**Corpus:**\n",
    "`\"I love ice cream\"`\n",
    "\n",
    "**Trigrams and probabilities:**\n",
    "\n",
    "* P(ice | I love) = 1\n",
    "* P(cream | love ice) = 1\n",
    "\n",
    "**To calculate a sentence probability (approximate):**\n",
    "- Assume we also have P(I) and P(love | I):\n",
    "- P(sentence) = P(I) Ã— P(love | I) Ã— P(ice | I love) Ã— P(cream | love ice)\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "\n",
    "| Model   | Memory  | Example Prediction                  |\n",
    "| ------- | ------- | ----------------------------------- |\n",
    "| Unigram | 0 words | Predict next word from overall freq |\n",
    "| Bigram  | 1 word  | Predict \"cream\" from \"ice\"          |\n",
    "| Trigram | 2 words | Predict \"cream\" from \"love ice\"     |\n",
    "\n",
    "---\n",
    "\n",
    "### Real Use Case Example:\n",
    "\n",
    "Suppose you're using a bigram model and want to predict the next word after **\"machine\"**.\n",
    "From a corpus, you might get:\n",
    "\n",
    "* P(learning | machine) = 0.7\n",
    "* P(gun | machine) = 0.2\n",
    "* P(shop | machine) = 0.1\n",
    "\n",
    "So, the model would most likely predict **\"learning\"**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c882007c",
   "metadata": {},
   "source": [
    "### **Bigram Model** step-by-step using the sentence:\n",
    "\n",
    "> **\"Ashi have ice cream jar. Ice cream is her fav.\"**\n",
    "\n",
    "---\n",
    "\n",
    "###  Step 1: **Preprocess the Sentence**\n",
    "\n",
    "Letâ€™s split the text into lowercase words and add a sentence boundary marker `<s>` at the beginning of each sentence.\n",
    "\n",
    "**Preprocessed tokens:**\n",
    "\n",
    "```\n",
    "<s> ashi have ice cream jar .\n",
    "<s> ice cream is her fav .\n",
    "```\n",
    "\n",
    "Tokenized list:\n",
    "\n",
    "```\n",
    "['<s>', 'ashi', 'have', 'ice', 'cream', 'jar', '.', '<s>', 'ice', 'cream', 'is', 'her', 'fav', '.']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  Step 2: **Extract Bigrams**\n",
    "\n",
    "From the token list, we form pairs of consecutive words:\n",
    "\n",
    "```\n",
    "('<s>', 'ashi')\n",
    "('ashi', 'have')\n",
    "('have', 'ice')\n",
    "('ice', 'cream')\n",
    "('cream', 'jar')\n",
    "('jar', '.')\n",
    "('.', '<s>')\n",
    "('<s>', 'ice')\n",
    "('ice', 'cream')\n",
    "('cream', 'is')\n",
    "('is', 'her')\n",
    "('her', 'fav')\n",
    "('fav', '.')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  Step 3: **Count Bigrams and Unigrams**\n",
    "\n",
    "#### Bigram Counts:\n",
    "\n",
    "| Bigram           | Count |\n",
    "| ---------------- | ----- |\n",
    "| ('<s>', 'ashi')  | 1     |\n",
    "| ('ashi', 'have') | 1     |\n",
    "| ('have', 'ice')  | 1     |\n",
    "| ('ice', 'cream') | 2     |\n",
    "| ('cream', 'jar') | 1     |\n",
    "| ('jar', '.')     | 1     |\n",
    "| ('.', '<s>')     | 1     |\n",
    "| ('<s>', 'ice')   | 1     |\n",
    "| ('cream', 'is')  | 1     |\n",
    "| ('is', 'her')    | 1     |\n",
    "| ('her', 'fav')   | 1     |\n",
    "| ('fav', '.')     | 1     |\n",
    "\n",
    "#### Unigram Counts (first word in each bigram):\n",
    "\n",
    "| Word  | Count |\n",
    "| ----- | ----- |\n",
    "| `<s>` | 2     |\n",
    "| ashi  | 1     |\n",
    "| have  | 1     |\n",
    "| ice   | 2     |\n",
    "| cream | 2     |\n",
    "| jar   | 1     |\n",
    "| .     | 1     |\n",
    "| is    | 1     |\n",
    "| her   | 1     |\n",
    "| fav   | 1     |\n",
    "\n",
    "---\n",
    "\n",
    "###  Step 4: **Calculate Bigram Probabilities**\n",
    "\n",
    "Using Maximum Likelihood Estimation (MLE):\n",
    "\n",
    "$$\n",
    "P(w_i \\mid w_{i-1}) = \\frac{\\text{Count}(w_{i-1}, w_i)}{\\text{Count}(w_{i-1})}\n",
    "$$\n",
    "\n",
    "Examples:\n",
    "\n",
    "* $P(ashi \\mid <s>) = \\frac{1}{2} = 0.5$\n",
    "* $P(ice \\mid have) = \\frac{1}{1} = 1.0$\n",
    "* $P(cream \\mid ice) = \\frac{2}{2} = 1.0$\n",
    "* $P(jar \\mid cream) = \\frac{1}{2} = 0.5$\n",
    "* $P(is \\mid cream) = \\frac{1}{2} = 0.5$\n",
    "\n",
    "---\n",
    "\n",
    "###  Step 5: **Bigram Sentence Probability**\n",
    "\n",
    "Letâ€™s compute the **probability of the first sentence**:\n",
    "**\"Ashi have ice cream jar .\"**\n",
    "With tokens: `<s> ashi have ice cream jar .`\n",
    "\n",
    "$$\n",
    "P(<s>, ashi, have, ice, cream, jar, .) = P(ashi \\mid <s>) \\times P(have \\mid ashi) \\times P(ice \\mid have) \\times P(cream \\mid ice) \\times P(jar \\mid cream) \\times P(. \\mid jar)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{2} \\times \\frac{1}{1} \\times \\frac{1}{1} \\times \\frac{2}{2} \\times \\frac{1}{2} \\times \\frac{1}{1} = 0.5 \\times 1 \\times 1 \\times 1 \\times 0.5 \\times 1 = **0.25**\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "###  Summary:\n",
    "\n",
    "The **bigram model** helps assign probabilities to word sequences by looking at **pairs of words**. In this example:\n",
    "\n",
    "* It recognizes that **\"ice cream\"** is more probable (seen twice).\n",
    "* It gives **lower probabilities** to less frequent bigrams like **\"cream jar\"** or **\"ashi have\"**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb4cfa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Sample text\n",
    "text = \"Ashi has a cat Doma Doma is very naughty Doma is not like other cats Ashi still loves cat Doma.\"\n",
    "\n",
    "# Tokenize text\n",
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "329e3aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Frequencies:\n",
      "('Ashi', 'has'): 1\n",
      "('has', 'a'): 1\n",
      "('a', 'cat'): 1\n",
      "('cat', 'Doma'): 2\n",
      "('Doma', 'Doma'): 1\n",
      "('Doma', 'is'): 2\n",
      "('is', 'very'): 1\n",
      "('very', 'naughty'): 1\n",
      "('naughty', 'Doma'): 1\n",
      "('is', 'not'): 1\n",
      "('not', 'like'): 1\n",
      "('like', 'other'): 1\n",
      "('other', 'cats'): 1\n",
      "('cats', 'Ashi'): 1\n",
      "('Ashi', 'still'): 1\n",
      "('still', 'loves'): 1\n",
      "('loves', 'cat'): 1\n",
      "('Doma', '.'): 1\n"
     ]
    }
   ],
   "source": [
    "# Generate bigrams\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "\n",
    "# Count frequency of bigrams\n",
    "bigram_freq = Counter(bigrams)\n",
    "\n",
    "print(\"Bigram Frequencies:\")\n",
    "for bg, freq in bigram_freq.items():\n",
    "    print(f\"{bg}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "719273d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Probabilities of words following 'cat':\n",
      "P(Doma | cat) = 1.00\n"
     ]
    }
   ],
   "source": [
    "# Estimate P(word2 | word1) where word1 is 'Doma'\n",
    "word1 = 'cat'\n",
    "following_words = {bg[1]: freq for bg, freq in bigram_freq.items() if bg[0] == word1}\n",
    "\n",
    "total_count = sum(following_words.values())\n",
    "probabilities = {word: count / total_count for word, count in following_words.items()}\n",
    "\n",
    "print(f\"\\nProbabilities of words following '{word1}':\")\n",
    "for word, prob in probabilities.items():\n",
    "    print(f\"P({word} | {word1}) = {prob:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edd9f29",
   "metadata": {},
   "source": [
    "## Simple Neural Network Language Model with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfcbd7c",
   "metadata": {},
   "source": [
    "\"life is love\"\n",
    "[1,2,3]\n",
    "[length=4]\n",
    "[0,1,2,3]\n",
    "[0,0,1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30f9d630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Suyashi144893\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given seed text: 'Ashi has a'\n",
      "Predicted next word: 'cat'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"Ashi has a cat Doma\",\n",
    "    \"Doma is very naughty\",\n",
    "    \"All cats are not like Doma\",\n",
    "    \"Ashi still loves her\"\n",
    "]\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Create input sequences\n",
    "input_sequences = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_seq = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_seq)\n",
    "\n",
    "# Padding\n",
    "max_seq_len = max(len(seq) for seq in input_sequences)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre')\n",
    "\n",
    "# Split X and y\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
    "\n",
    "# Build model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(total_words, 10, input_length=max_seq_len - 1),\n",
    "    tf.keras.layers.SimpleRNN(50),\n",
    "    tf.keras.layers.Dense(total_words, activation='softmax')\n",
    "])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "\n",
    "# Predict next word\n",
    "seed_text = \"Ashi has a\"\n",
    "token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "token_list = pad_sequences([token_list], maxlen=max_seq_len - 1, padding='pre')\n",
    "\n",
    "predicted_probs = model.predict(token_list, verbose=0)\n",
    "predicted_index = np.argmax(predicted_probs, axis=1)[0]\n",
    "predicted_word = tokenizer.index_word[predicted_index]\n",
    "\n",
    "print(f\"Given seed text: '{seed_text}'\")\n",
    "print(f\"Predicted next word: '{predicted_word}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6ccf2a",
   "metadata": {},
   "source": [
    "## Full Bigram Model Code with Sentence Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed27330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Step 1: Preprocess the Text\n",
    "text = \"Ashi have ice cream jar. Ice cream is her fav.\"\n",
    "text = text.lower()\n",
    "sentences = re.split(r'[.!?]', text)\n",
    "tokens = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = sentence.strip().split()\n",
    "    if words:\n",
    "        tokens += ['<s>'] + words + ['</s>']\n",
    "\n",
    "# Step 2: Count Unigrams and Bigrams\n",
    "bigram_counts = defaultdict(int)\n",
    "unigram_counts = defaultdict(int)\n",
    "\n",
    "for i in range(len(tokens) - 1):\n",
    "    unigram_counts[tokens[i]] += 1\n",
    "    bigram_counts[(tokens[i], tokens[i + 1])] += 1\n",
    "\n",
    "unigram_counts[tokens[-1]] += 1  # last word\n",
    "\n",
    "# Step 3: Calculate Bigram Probabilities\n",
    "bigram_prob = {}\n",
    "for (w1, w2), count in bigram_counts.items():\n",
    "    prob = count / unigram_counts[w1]\n",
    "    bigram_prob[(w1, w2)] = round(prob, 3)\n",
    "\n",
    "# Step 4: Show Bigram Probabilities\n",
    "print(\"Bigram Probabilities:\\n\")\n",
    "for (w1, w2), prob in bigram_prob.items():\n",
    "    print(f\"P({w2} | {w1}) = {prob}\")\n",
    "\n",
    "# Step 5: Calculate Sentence Probability\n",
    "def sentence_probability(sentence, bigram_prob_dict):\n",
    "    words = ['<s>'] + sentence.lower().split() + ['</s>']\n",
    "    prob = 1.0\n",
    "    for i in range(len(words) - 1):\n",
    "        pair = (words[i], words[i + 1])\n",
    "        prob *= bigram_prob_dict.get(pair, 0)\n",
    "    return prob\n",
    "\n",
    "# Test Sentence\n",
    "test_sentence = \"ashi have ice cream jar\"\n",
    "prob = sentence_probability(test_sentence, bigram_prob)\n",
    "print(f\"\\nSentence: '{test_sentence}'\")\n",
    "print(f\"Bigram Model Probability: {round(prob, 5)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
