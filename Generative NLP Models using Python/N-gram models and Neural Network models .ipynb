{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f30447d",
   "metadata": {},
   "source": [
    "\n",
    "### N-gram Models\n",
    "\n",
    "N-gram models are a type of probabilistic language model that predicts the next word based on the previous (N-1) words. For example, a bigram (2-gram) looks at the previous one word, a trigram (3-gram) looks at the previous two words, etc.\n",
    "\n",
    "### How they work?\n",
    " N-gram models use a **probabilistic formula** based on conditional probabilities. Here’s a breakdown of the core **formula** and related concepts:\n",
    "\n",
    "---\n",
    "\n",
    "###  **General N-gram Formula**\n",
    "\n",
    "For an N-gram model, the **probability of a word sequence** is approximated by:\n",
    "\n",
    "$$\n",
    "P(w_1, w_2, ..., w_n) \\approx \\prod_{i=1}^{n} P(w_i \\mid w_{i-(N-1)}, ..., w_{i-1})\n",
    "$$\n",
    "\n",
    "This means:\n",
    "The probability of a word $w_i$ depends only on the previous $N-1$ words.\n",
    "\n",
    "---\n",
    "\n",
    "### For Specific N-values:\n",
    "\n",
    "#### ➤ **Unigram Model (N = 1)**\n",
    "\n",
    "Assumes words occur independently:\n",
    "\n",
    "$$\n",
    "P(w_1, w_2, w_3) = P(w_1) \\cdot P(w_2) \\cdot P(w_3)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### ➤ **Bigram Model (N = 2)**\n",
    "\n",
    "Each word depends on the **previous one**:\n",
    "\n",
    "$$\n",
    "P(w_1, w_2, w_3) \\approx P(w_1) \\cdot P(w_2 \\mid w_1) \\cdot P(w_3 \\mid w_2)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "P(w_n \\mid w_{n-1}) = \\frac{\\text{Count}(w_{n-1}, w_n)}{\\text{Count}(w_{n-1})}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### ➤ **Trigram Model (N = 3)**\n",
    "\n",
    "Each word depends on the **previous two**:\n",
    "\n",
    "$$\n",
    "P(w_3 \\mid w_1, w_2) = \\frac{\\text{Count}(w_1, w_2, w_3)}{\\text{Count}(w_1, w_2)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Example (Bigram):\n",
    "\n",
    "Here's a simple visual representation of the **Bigram Probability Calculation** using the example `\"Ashi has a cat Doma\"`:\n",
    "\n",
    "---\n",
    "\n",
    "### Sentence Breakdown:\n",
    "\n",
    "```\n",
    "\"Ashi has a cat Doma\"\n",
    "```\n",
    "\n",
    "| Word 1 | Word 2 | Bigram      |\n",
    "| ------ | ------ | ----------- |\n",
    "| Ashi   | has    | (Ashi, has) |\n",
    "| has    | a      | (has, a)    |\n",
    "| a      | cat    | (a, cat)    |\n",
    "| cat    | Doma   | (cat, Doma) |\n",
    "\n",
    "---\n",
    "\n",
    "### Bigram Frequencies (from a larger corpus):\n",
    "\n",
    "| Bigram                 | Count                                      |\n",
    "| ---------------------- | ------------------------------------------ |\n",
    "| (has, a)               | 2                                          |\n",
    "| (has, cat)             | 1                                          |\n",
    "| (has, food)            | 0                                          |\n",
    "| TOTAL \"has\" precedents | 3 (sum of all bigrams starting with \"has\") |\n",
    "\n",
    "---\n",
    "\n",
    "###  Bigram Probability Formula:\n",
    "\n",
    "$$\n",
    "P(\\text{\"a\"} \\mid \\text{\"has\"}) = \\frac{\\text{Count}(\\text{\"has\"}, \\text{\"a\"})}{\\text{Count}(\\text{\"has\"})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{2}{3} \\approx 0.67\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Visual Summary:\n",
    "\n",
    "```\n",
    "\"has\" → \n",
    "     ├── \"a\" (2 times)        → P = 2/3\n",
    "     └── \"cat\" (1 time)       → P = 1/3\n",
    "```\n",
    "\n",
    "This tells us:\n",
    "\n",
    "> If the model sees the word `\"has\"`, it's **more likely** (67%) to predict `\"a\"` next than `\"cat\"` (33%).\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to generate a **diagram-style graphic** of this as an image?\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Smoothing (Optional but Important)\n",
    "\n",
    "In real-world texts, some N-grams might not appear (zero counts). This can cause probability to be 0. To avoid this, use **smoothing techniques**:\n",
    "\n",
    "* **Add-One (Laplace) Smoothing**\n",
    "\n",
    "$$\n",
    "P(w_n \\mid w_{n-1}) = \\frac{\\text{Count}(w_{n-1}, w_n) + 1}{\\text{Count}(w_{n-1}) + V}\n",
    "$$\n",
    "\n",
    "Where $V$ = vocabulary size.\n",
    "\n",
    "* **Good-Turing Smoothing**\n",
    "* **Kneser-Ney Smoothing** (advanced, used in real NLP systems)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Advantages \n",
    "\n",
    "- Simple and interpretable\n",
    "\n",
    "- Easy to implement\n",
    "\n",
    "- Works reasonably well on small datasets or for specific domains\n",
    "\n",
    "### Limitations:\n",
    "\n",
    "- Struggles with long-range dependencies (only looks at fixed N words)\n",
    "\n",
    "- Suffers from data sparsity: many N-grams may never appear in training data, requiring smoothing techniques\n",
    "\n",
    "- Does not capture semantic meaning well — just relies on frequency counts\n",
    "\n",
    "### Neural Network Models\n",
    "\n",
    "Neural network models for language (e.g., RNNs, LSTMs, Transformers) are deep learning models that learn word representations and patterns in text data through multiple layers of nonlinear transformations.\n",
    "\n",
    "### How they work ?\n",
    "Instead of relying on fixed counts, these models learn dense vector embeddings of words that capture semantic and syntactic properties. For example, a Transformer model can attend to all words in a sentence, capturing complex relationships and context over long distances.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- Capture long-range context and dependencies well\n",
    "\n",
    "- Learn meaningful word embeddings that capture semantic similarity\n",
    "\n",
    "- Handle complex patterns and generalize better to unseen text\n",
    "\n",
    "- State-of-the-art performance in many NLP tasks\n",
    "\n",
    "### Limitations:\n",
    "\n",
    "- Require large amounts of data and computational power\n",
    "\n",
    "- Less interpretable than N-gram models\n",
    "\n",
    "- Can be complex to train and tune\n",
    "\n",
    "### An N-gram language model using the nltk library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "719273d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Frequencies:\n",
      "('Ashi', 'has'): 1\n",
      "('has', 'a'): 1\n",
      "('a', 'cat'): 1\n",
      "('cat', 'Doma'): 1\n",
      "('Doma', '.'): 2\n",
      "('.', 'Doma'): 1\n",
      "('Doma', 'is'): 1\n",
      "('is', 'very'): 1\n",
      "('very', 'naughty'): 1\n",
      "('naughty', '.'): 1\n",
      "('.', 'All'): 1\n",
      "('All', 'cats'): 1\n",
      "('cats', 'are'): 1\n",
      "('are', 'not'): 1\n",
      "('not', 'like'): 1\n",
      "('like', 'Doma'): 1\n",
      "('.', 'Ashi'): 1\n",
      "('Ashi', 'still'): 1\n",
      "('still', 'loves'): 1\n",
      "('loves', 'her'): 1\n",
      "('her', '.'): 1\n",
      "\n",
      "Probabilities of words following 'Doma':\n",
      "P(. | Doma) = 0.67\n",
      "P(is | Doma) = 0.33\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Sample text\n",
    "text = \"Ashi has a cat Doma. Doma is very naughty. All cats are not like Doma. Ashi still loves her.\"\n",
    "\n",
    "# Tokenize text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Generate bigrams\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "\n",
    "# Count frequency of bigrams\n",
    "bigram_freq = Counter(bigrams)\n",
    "\n",
    "print(\"Bigram Frequencies:\")\n",
    "for bg, freq in bigram_freq.items():\n",
    "    print(f\"{bg}: {freq}\")\n",
    "\n",
    "# Estimate P(word2 | word1) where word1 is 'Doma'\n",
    "word1 = 'Doma'\n",
    "following_words = {bg[1]: freq for bg, freq in bigram_freq.items() if bg[0] == word1}\n",
    "\n",
    "total_count = sum(following_words.values())\n",
    "probabilities = {word: count / total_count for word, count in following_words.items()}\n",
    "\n",
    "print(f\"\\nProbabilities of words following '{word1}':\")\n",
    "for word, prob in probabilities.items():\n",
    "    print(f\"P({word} | {word1}) = {prob:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edd9f29",
   "metadata": {},
   "source": [
    "## Simple Neural Network Language Model with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30f9d630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given seed text: 'Ashi has a'\n",
      "Predicted next word: 'cat'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"Ashi has a cat Doma\",\n",
    "    \"Doma is very naughty\",\n",
    "    \"All cats are not like Doma\",\n",
    "    \"Ashi still loves her\"\n",
    "]\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Create input sequences\n",
    "input_sequences = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_seq = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_seq)\n",
    "\n",
    "# Padding\n",
    "max_seq_len = max(len(seq) for seq in input_sequences)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre')\n",
    "\n",
    "# Split X and y\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
    "\n",
    "# Build model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(total_words, 10, input_length=max_seq_len - 1),\n",
    "    tf.keras.layers.SimpleRNN(50),\n",
    "    tf.keras.layers.Dense(total_words, activation='softmax')\n",
    "])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "\n",
    "# Predict next word\n",
    "seed_text = \"Ashi has a\"\n",
    "token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "token_list = pad_sequences([token_list], maxlen=max_seq_len - 1, padding='pre')\n",
    "\n",
    "predicted_probs = model.predict(token_list, verbose=0)\n",
    "predicted_index = np.argmax(predicted_probs, axis=1)[0]\n",
    "predicted_word = tokenizer.index_word[predicted_index]\n",
    "\n",
    "print(f\"Given seed text: '{seed_text}'\")\n",
    "print(f\"Predicted next word: '{predicted_word}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed27330c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
