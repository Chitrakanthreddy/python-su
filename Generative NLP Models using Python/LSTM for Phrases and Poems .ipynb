{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This project demonstrates how to use an LSTM (Long Short-Term Memory) neural network for generating meaningful text based on user input. It also includes temperature control to affect the creativity of the output text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1. `import numpy as np`\n",
    "\n",
    "* **Numpy** is a core library for numerical computing in Python.\n",
    "* `np` is the commonly used alias for numpy.\n",
    "* Useful functions: arrays, math operations, random generation, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `import tensorflow as tf`\n",
    "\n",
    "* **TensorFlow** is an open-source library for machine learning and deep learning.\n",
    "* `tf` is the standard alias.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. `from tensorflow.keras.models import Sequential`\n",
    "\n",
    "* Imports the **Sequential model class** from Keras.\n",
    "* It is used to **build models layer by layer**, where each layer has exactly one input tensor and one output tensor.\n",
    "\n",
    " \n",
    "### 4. `from tensorflow.keras.layers import LSTM, Dense, Embedding`\n",
    "\n",
    "These are **types of layers** that can be added to a Sequential model:\n",
    "\n",
    "#### `LSTM`:\n",
    "\n",
    "* Long Short-Term Memory layer.\n",
    "* A type of RNN (Recurrent Neural Network) used to process **sequential data** like text or time series.\n",
    "* Captures long-term dependencies in sequences.\n",
    "\n",
    "#### `Dense`:\n",
    "\n",
    "* Fully connected layer.\n",
    "* Each input node connects to each output node.\n",
    "* Used at the end of the network (e.g., for classification or regression output).\n",
    "\n",
    "#### `Embedding`:\n",
    "\n",
    "* Used in NLP.\n",
    "* Converts **integer-encoded words** into dense vectors of fixed size (word embeddings).\n",
    "* First layer often used in NLP models.\n",
    "\n",
    "\n",
    "\n",
    "### 5. `from tensorflow.keras.preprocessing.sequence import pad_sequences`\n",
    "\n",
    "* `pad_sequences` is used to **ensure all sequences have the same length** by:\n",
    "\n",
    "  * Padding shorter sequences with zeros (or another value).\n",
    "  * Truncating longer sequences.\n",
    "\n",
    "\n",
    "\n",
    "### 6. `from tensorflow.keras.preprocessing.text import Tokenizer`\n",
    "\n",
    "* `Tokenizer` is used to:\n",
    "\n",
    "  * Vectorize a text corpus.\n",
    "  * Convert text into sequences of integers (word indices).\n",
    "  * Create a word index dictionary.\n",
    "\n",
    "\n",
    "### 7. `import random`\n",
    "\n",
    "* Python’s built-in **random module**.\n",
    "* Used for generating random numbers, selecting random items, etc.\n",
    "\n",
    "\n",
    "### Summary Table:\n",
    "\n",
    "| Component       | Purpose                             |\n",
    "| --------------- | ----------------------------------- |\n",
    "| `np`            | Numerical computing with arrays     |\n",
    "| `tf`            | Deep learning with TensorFlow       |\n",
    "| `Sequential`    | Model architecture builder          |\n",
    "| `LSTM`          | Sequence learning (e.g., text/time) |\n",
    "| `Dense`         | Fully connected layer               |\n",
    "| `Embedding`     | Word embedding (vectorizing text)   |\n",
    "| `pad_sequences` | Equal-length sequence padding       |\n",
    "| `Tokenizer`     | Convert text to numerical tokens    |\n",
    "| `random`        | General-purpose randomness          |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3],\n",
       "       [0, 0, 4, 5]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences([[1, 2, 3], [4, 5]], maxlen=4)\n",
    "\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature \n",
    "\n",
    "### Temperature is a tuning parameter used to control the randomness of predictions in text generation. It changes the distribution of predicted next words before sampling.\n",
    "- min value of temp is close to 0 (e.g.0.2)\n",
    "- max value of temp can be any positive value, but most of the time it is set between 1 and 2. \n",
    "\n",
    "hello: 0.6  \n",
    "world: 0.2  \n",
    "this: 0.1  \n",
    "deep: 0.1\n",
    "\n",
    "If temperature = 1.0 (default):\n",
    "Keeps probabilities as they are\n",
    "\n",
    "Sampling is balanced and creative\n",
    "\n",
    "#### If temperature = 0.1 (low):\n",
    "- Sharpens the distribution\n",
    "- Makes the model very confident — selects high-probability words\n",
    "\n",
    "\n",
    "#### If temperature = 1.5 (high):\n",
    "- Flattens the distribution\n",
    "- Makes the model more random — even low-probability words can be picked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematically:\n",
    "\n",
    "preds = np.log(preds + 1e-8) / temperature\n",
    "\n",
    "- Low temperature ⇒ logits (scores before softmax) become sharper ⇒ top prediction dominates.\n",
    "\n",
    "- High temperature ⇒ logits get flatter ⇒ more exploration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Suyashi144893\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x241fd545c50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Corpus (Poetic)\n",
    "corpus = [\n",
    "    \"This world a canvas vast and wide\",\n",
    "    \"Where dreams and hopes in hearts reside\",\n",
    "    \"With every dawn a chance to grow\",\n",
    "    \"In sunlit fields or moonlit glow\",\n",
    "    \"Through trials faced and joys embraced\",\n",
    "    \"We find our path our steps retraced\",\n",
    "    \"In unity we rise and stand\",\n",
    "    \"Together we shape this wondrous land\"\n",
    "]\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Create input sequences\n",
    "input_sequences = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line.lower()])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Padding\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
    "input_sequences = np.array(input_sequences)\n",
    "\n",
    "# Predictors and labels\n",
    "X = input_sequences[:, :-1]\n",
    "y = tf.keras.utils.to_categorical(input_sequences[:, -1], num_classes=total_words)\n",
    "\n",
    "# Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 10, input_length=max_sequence_len - 1))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, verbose=0)  # Train silently\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature-Based Text Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_temperature(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype(\"float64\")\n",
    "    preds = np.log(preds + 1e-8) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len, temperature=1.0):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text.lower()])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
    "        predictions = model.predict(token_list, verbose=0)[0]\n",
    "        predicted_index = sample_with_temperature(predictions, temperature)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_index:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp 0.2: in sunlit fields or moonlit glow wide reside grow\n",
      "Temp 0.7: in unity we rise and stand embraced land retraced reside reside\n",
      "Temp 1.2: in unity we or and stand embraced land retraced reside reside hearts\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"in\"\n",
    "print(\"Temp 0.2:\", generate_text(seed_text, next_words=8, model=model, max_sequence_len=max_sequence_len, temperature=0.2))\n",
    "print(\"Temp 0.7:\", generate_text(seed_text, next_words=10, model=model, max_sequence_len=max_sequence_len, temperature=0.7))\n",
    "print(\"Temp 1.2:\", generate_text(seed_text, next_words=11, model=model, max_sequence_len=max_sequence_len, temperature=1.2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rhyming Enhancement\n",
    "We can post-process the generated text to rhyme the last word of each line. One way is to use the pronouncing library (based on CMU Pronouncing Dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pip install pronouncing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x24185b81690>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pronouncing\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Poetic Corpus\n",
    "corpus = [\n",
    "    \"This world a canvas vast and wide\",\n",
    "    \"Where dreams and hopes in hearts reside\",\n",
    "    \"With every dawn a chance to grow\",\n",
    "    \"In sunlit fields or moonlit glow\",\n",
    "    \"Through trials faced and joys embraced\",\n",
    "    \"We find our path our steps retraced\",\n",
    "    \"In unity we rise and stand\",\n",
    "    \"Together we shape this wondrous land\"\n",
    "]\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Input Sequences\n",
    "input_sequences = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line.lower()])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Padding\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
    "input_sequences = np.array(input_sequences)\n",
    "\n",
    "# X and y\n",
    "X = input_sequences[:, :-1]\n",
    "y = tf.keras.utils.to_categorical(input_sequences[:, -1], num_classes=total_words)\n",
    "\n",
    "# Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 10, input_length=max_sequence_len - 1))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model.fit(X, y, epochs=300, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation with Temperature & Rhyming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature Sampling\n",
    "def sample_with_temperature(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype(\"float64\")\n",
    "    preds = np.log(preds + 1e-8) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    return np.random.choice(len(preds), p=preds)\n",
    "\n",
    "# Format as Verse\n",
    "def format_as_verse(text, words_per_line=5):\n",
    "    words = text.strip().split()\n",
    "    return \"\\n\".join([\" \".join(words[i:i+words_per_line]) for i in range(0, len(words), words_per_line)])\n",
    "\n",
    "# Generate Text\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len, temperature=1.0, rhyme_last_word=None):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text.lower()])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
    "        predictions = model.predict(token_list, verbose=0)[0]\n",
    "\n",
    "        # Sample word with temperature\n",
    "        next_index = sample_with_temperature(predictions, temperature)\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == next_index:\n",
    "                output_word = word\n",
    "                break\n",
    "\n",
    "        seed_text += \" \" + output_word\n",
    "\n",
    "    # Format\n",
    "    lines = format_as_verse(seed_text).split(\"\\n\")\n",
    "\n",
    "    # Rhyme adjustment (optional: only on last word of every 2nd line)\n",
    "    if rhyme_last_word:\n",
    "        last_words = [line.split()[-1] for line in lines]\n",
    "        rhymes = pronouncing.rhymes(rhyme_last_word)\n",
    "        for i in range(1, len(lines), 2):\n",
    "            if i < len(lines):\n",
    "                new_last_word = next((r for r in rhymes if r in tokenizer.word_index), None)\n",
    "                if new_last_word:\n",
    "                    words = lines[i].split()\n",
    "                    words[-1] = new_last_word\n",
    "                    lines[i] = \" \".join(words)\n",
    "    return \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a Rhymed Poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Poem:\n",
      "\n",
      "with every dawn a chance\n",
      "to grow grow wide grow\n",
      "wide wide wide wide hearts\n",
      "hearts hearts wide reside grow\n",
      "reside reside\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"with every\"\n",
    "output = generate_text(seed_text, next_words=20, model=model, max_sequence_len=max_sequence_len, temperature=0.8, rhyme_last_word=\"glow\")\n",
    "print(\"Generated Poem:\\n\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rhyming Word Injection Too Early\n",
    "- Cause: The rhyme_last_word is being introduced too early or overriding natural generation.\n",
    "\n",
    "- Fix:Apply rhyming correction after generation, only to the last word of each line.\n",
    "\n",
    "### Add Filtering for Repeats\n",
    "Add a small logic to block the same word from being repeated too many times in a row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_repetitions(text):\n",
    "    words = text.split()\n",
    "    filtered = []\n",
    "    for i, word in enumerate(words):\n",
    "        if i >= 2 and word == words[i-1] == words[i-2]:\n",
    "            continue  # Skip word if repeated 3 times\n",
    "        filtered.append(word)\n",
    "    return \" \".join(filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poem(seed_text, next_words, model, max_sequence_len, temperature=1.0, words_per_line=5, rhyme_every=2):\n",
    "    text = seed_text\n",
    "\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([text.lower()])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
    "        predictions = model.predict(token_list, verbose=0)[0]\n",
    "\n",
    "        next_index = sample_with_temperature(predictions, temperature)\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == next_index:\n",
    "                output_word = word\n",
    "                break\n",
    "\n",
    "        text += \" \" + output_word\n",
    "\n",
    "    # Filter repetitive patterns\n",
    "    text = filter_repetitions(text)\n",
    "\n",
    "    # Format as lines\n",
    "    lines = format_as_verse(text, words_per_line=words_per_line).split(\"\\n\")\n",
    "\n",
    "    # Rhyming adjustment for every nth line\n",
    "    for i in range(rhyme_every - 1, len(lines), rhyme_every):\n",
    "        last_word = lines[i].split()[-1]\n",
    "        rhymes = pronouncing.rhymes(last_word)\n",
    "        if rhymes:\n",
    "            candidate = next((r for r in rhymes if r in tokenizer.word_index), None)\n",
    "            if candidate:\n",
    "                words = lines[i].split()\n",
    "                words[-1] = candidate\n",
    "                lines[i] = \" \".join(words)\n",
    "\n",
    "    return \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Poem:\n",
      "\n",
      "through trials faced and joys\n",
      "embraced reside reside steps embraced\n",
      "hearts wide land wide wide\n",
      "retraced hearts land land reside\n",
      "wide retraced wide hearts wide\n",
      "wide reside wide\n"
     ]
    }
   ],
   "source": [
    "poem = generate_poem(\"through trials\", next_words=30, model=model, max_sequence_len=max_sequence_len, temperature=1.2)\n",
    "print(\"Generated Poem:\\n\")\n",
    "print(poem)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
