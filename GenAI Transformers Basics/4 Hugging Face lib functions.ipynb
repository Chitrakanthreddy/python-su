{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6ca2da3",
   "metadata": {},
   "source": [
    "### The Hugging Face transformers library provides a variety of pretrained models and pipelines that can perform different natural language processing (NLP) tasks beyond sentiment analysis. Below are some examples of tasks you can perform using different pipelines provided by the transformers library, along with a set of functions demonstrating these capabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4bb5d7",
   "metadata": {},
   "source": [
    "pip install wordcloud --trusted-host pypi.org --trusted-host files.pythonhosted.org transformers==4.9.2 torch==1.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "891a4217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9996954202651978}]\n"
     ]
    }
   ],
   "source": [
    "#1. Text Classification (other than sentiment analysis)\n",
    "#Function: Classify text into predefined categories.\n",
    "from transformers import pipeline\n",
    "# Explicitly specify the model name\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "def classify_text(text, model_name=\"distilbert-base-uncased-finetuned-sst-2-english\"):\n",
    "    classifier = pipeline(\"text-classification\", model=model_name)\n",
    "    results = classifier(text)\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "text = \"I hated the movie\"\n",
    "print(classify_text(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb5cf549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47be53d6e247411a92aeca8ee35e3bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d168a3de5954840a405bdab85f97419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28cbb30d66274c9ba6266e8f5396cd64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3262b62c01ea4feeb27c2d096ac1dcd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suyashi144893\\Anaconda3\\lib\\site-packages\\transformers\\pipelines\\token_classification.py:154: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9957955, 'word': 'Ashi', 'start': 0, 'end': 4}, {'entity_group': 'LOC', 'score': 0.99833214, 'word': 'Delhi', 'start': 16, 'end': 21}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def recognize_entities(text):\n",
    "    model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "    ner_pipeline = pipeline(\"ner\", model=model_name, grouped_entities=True)\n",
    "    results = ner_pipeline(text)\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "text = \"Ashi is born in Delhi.\"\n",
    "print(recognize_entities(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c25bdd16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c0a7858a6b94e6ba5ce781669ee3f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038c147c34f7464080e05777e88d4cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f47b75b676247f5b9b2f939790df927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92cf74bff7a041ed99021f050de40623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d70526834ae49f2a985758436686cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.975246012210846, 'start': 40, 'end': 53, 'answer': 'New York City'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def answer_question(question, context):\n",
    "    model_name = \"distilbert-base-cased-distilled-squad\"\n",
    "    qa_pipeline = pipeline(\"question-answering\", model=model_name)\n",
    "    results = qa_pipeline(question=question, context=context)\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "context = \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very close to the Manhattan Bridge.\"\n",
    "question = \"Where is Hugging Face Inc. based?\"\n",
    "print(answer_question(question, context))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c605f305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09819dcbd13946198e9d8af15de3fc06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c44f0cee68854a23be354e6d0850f524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f174441908344deaa2cb629f9eec9a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70c24fab64d4f8eb41e7e037babcdde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9bd561fd8064e49a3d8903a9dee97da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suyashi144893\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ..\\aten\\src\\ATen\\native\\BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A good human embodies kindness, empathy, and integrity. They act selflessly, helping others and showing compassion. Honesty and respect guide their interactions, fostering trust and positive relationships.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def summarize_text(text, max_length=50, min_length=25):\n",
    "    model_name = \"facebook/bart-large-cnn\"\n",
    "    summarization_pipeline = pipeline(\"summarization\", model=model_name)\n",
    "    summary = summarization_pipeline(text, max_length=max_length, min_length=min_length, do_sample=False)\n",
    "    return summary[0]['summary_text']\n",
    "\n",
    "# Example usage\n",
    "text = \"A good human embodies kindness, empathy, and integrity. They act selflessly, helping others and showing compassion. Honesty and respect guide their interactions, fostering trust and positive relationships. A good human values diversity, promotes equality, and strives to make the world a better place through their actions and understanding.\"\n",
    "print(summarize_text(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c8103f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter initial text or topic:Language\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language by the Book Society of America.\n",
      "\n",
      "It's so nice to be able to be so transparent about how we are going to use our power.\n",
      "\n",
      "I've been very lucky the time is right now. I have lots of work to\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def generate_text(prompt, max_length=50):\n",
    "    model_name = \"gpt2\"\n",
    "    text_generator = pipeline(\"text-generation\", model=model_name)\n",
    "    generated_text = text_generator(prompt, max_length=max_length, num_return_sequences=1)\n",
    "    return generated_text[0]['generated_text']\n",
    "\n",
    "# Example usage\n",
    "prompt = input(\"Enter initial text or topic:\")\n",
    "print(generate_text(prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f79f31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def translate_text(text, source_lang=\"en\", target_lang=\"fr\"):\n",
    "    model_name = f\"Helsinki-NLP/opus-mt-{source_lang}-{target_lang}\"\n",
    "    translation_pipeline = pipeline(\"translation\", model=model_name)\n",
    "    translation = translation_pipeline(text)\n",
    "    return translation[0]['translation_text']\n",
    "\n",
    "# Example usage\n",
    "text = \"A pretty cat\"\n",
    "print(translate_text(text, source_lang=\"en\", target_lang=\"fr\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4bb0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install or update required libraries\n",
    "pip install wordcloud --trusted-ho st pypi.org --trusted-host files.pythonhosted.org --upgrade datasets transformers huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bc34b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_dataset, load_metric\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n",
    "\n",
    "# Tokenize the input data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Create a data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Create a Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4275834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
